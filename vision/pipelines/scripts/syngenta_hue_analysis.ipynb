{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/fruitspec-lab/FruitSpec/Code/fsCounter')\n",
    "from omegaconf import OmegaConf\n",
    "import pyzed.sl as sl\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import kornia as K\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "from vision.misc.help_func import get_repo_dir, scale_dets, validate_output_path, scale\n",
    "from vision.depth.zed.svo_operations import get_frame, get_depth, get_point_cloud, get_dimensions, sl_get_dimensions\n",
    "\n",
    "repo_dir = get_repo_dir()\n",
    "sys.path.append(os.path.join(repo_dir, 'vision', 'detector', 'yolo_x'))\n",
    "\n",
    "from vision.pipelines.detection_flow import counter_detection\n",
    "from vision.pipelines.misc.filters import filter_by_distance, filter_by_size, filter_by_height, sort_out\n",
    "from vision.tracker.fsTracker.score_func import compute_dist_on_vec\n",
    "from vision.data.results_collector import ResultsCollector\n",
    "from vision.tools.translation import translation as T\n",
    "from vision.tools.camera import is_sturated\n",
    "from vision.tools.color import get_hue\n",
    "from vision.tools.video_wrapper import video_wrapper\n",
    "from vision.tools.image_stitching import plot_2_imgs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kornia as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = get_repo_dir()\n",
    "pipeline_config = \"/home/fruitspec-lab/FruitSpec/Code/fsCounter/vision/pipelines/config/pipeline_config.yaml\"\n",
    "runtime_config = \"/home/fruitspec-lab/FruitSpec/Code/fsCounter/vision/pipelines/config/runtime_config.yaml\"\n",
    "cfg = OmegaConf.load(pipeline_config)\n",
    "args = OmegaConf.load(runtime_config)\n",
    "\n",
    "validate_output_path(args.output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = counter_detection(cfg, args)\n",
    "results_collector = ResultsCollector(rotate=args.rotate)\n",
    "translation = T(cfg.translation.translation_size, cfg.translation.dets_only, cfg.translation.mode)\n",
    "\n",
    "cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_path = \"/home/fruitspec-lab/Downloads/tomato/analysis/230123/pre/5/measures_pix_size_mean_hue_depth.csv\"\n",
    "measures_frame = pd.read_csv(measures_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(images, nrows, ncols, titles=None, figsize=None, xlabels=None, ylabels=None, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Plots a grid of images using matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    images (list): A list of numpy arrays representing the images to be plotted.\n",
    "    nrows (int): The number of rows in the grid.\n",
    "    ncols (int): The number of columns in the grid.\n",
    "    titles (list, optional): A list of strings representing the titles of the images. Must have the same length as images.\n",
    "    figsize (tuple, optional): A tuple representing the size of the figure. Defaults to (ncols * 5, nrows * 5).\n",
    "    xlabels (list, optional): A list of strings representing the x-axis labels for each image. Must have the same length as images.\n",
    "    ylabels (list, optional): A list of strings representing the y-axis labels for each image. Must have the same length as images.\n",
    "    cmap (str, optional): The color map to use when plotting the images. Defaults to 'viridis'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    if isinstance(figsize, type(None)):\n",
    "        figsize = (ncols * 5, nrows * 5)\n",
    "    titles, xlabels, ylabels = (np.full(n_images, \"\") if isinstance(titles, type(None)) else arr\n",
    "                                for arr in [titles, xlabels, ylabels])\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        image = images[i]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel(xlabels[i])\n",
    "        ax.set_ylabel(ylabels[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cut_center_of_box(image, margin=0.05):\n",
    "    \"\"\"\n",
    "    Cuts the center of the box.\n",
    "    \n",
    "    Args:\n",
    "    - image: A 3D Numpy array representing a cropped xyz image.\n",
    "    - margin: A float representing the percentage of margin to remove from the center of the image.\n",
    "\n",
    "    Returns:\n",
    "    - A 3D Numpy array representing the cropped image with the outside of the box removed.\n",
    "    \"\"\"\n",
    "    t, l, (b, r) = 0 ,0, image.shape[:2]\n",
    "    y_max, x_max = image.shape[:2]\n",
    "    h_m = int((b-t)*margin)\n",
    "    w_m = int((r-l)*margin)\n",
    "    cut_box = image[max(0, t+h_m):min(y_max, b-h_m), max(0, l+w_m):min(x_max, r-w_m)]\n",
    "    return cut_box\n",
    "\n",
    "\n",
    "def xyz_center_of_box(image, method=\"median\"):\n",
    "    \"\"\"\n",
    "    Calculates the median or mean x, y, and z coordinates of the fruit.\n",
    "\n",
    "    Args:\n",
    "    - image: A 3D Numpy array representing a cropped xyz image.\n",
    "    - method: A string representing the method to use to calculate the center of the fruit. Must be either \"median\" or \"mean\".\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of floats representing the x, y, and z coordinates of the center of the fruit.\n",
    "    \"\"\"\n",
    "    if method==\"median\":\n",
    "        cut_box = cut_center_of_box(image, 0.025)\n",
    "        x_median = np.nanmedian(cut_box[:, :, 0])\n",
    "        y_median = np.nanmedian(cut_box[:, :, 1])\n",
    "        z_median = np.nanmedian(cut_box[:, :, 2])\n",
    "    elif method==\"mean\":\n",
    "        cut_box = cut_center_of_box(image, 0.4)\n",
    "        x_median = np.nanmean(cut_box[:, :, 0])\n",
    "        y_median = np.nanmean(cut_box[:, :, 1])\n",
    "        z_median = np.nanmean(cut_box[:, :, 2])\n",
    "    else: # calculates only on the edge of the cut box\n",
    "        cut_box = cut_center_of_box(image, 0.25).copy()\n",
    "        if cut_box.shape[0] > 10 and cut_box.shape[1] > 10:\n",
    "            cut_box[5:-5,5:-5] = np.nan\n",
    "        x_median = np.nanmedian(cut_box[:, :, 0])\n",
    "        y_median = np.nanmedian(cut_box[:, :, 1])\n",
    "        z_median = np.nanmedian(cut_box[:, :, 2])\n",
    "    return x_median, y_median, z_median\n",
    "\n",
    "\n",
    "def dist_to_box_center(image, method=\"median\"):\n",
    "    \"\"\"\n",
    "    Calculates the distance from the camera to the center of the fruit.\n",
    "\n",
    "    Args:\n",
    "    - image: A 3D Numpy array representing a cropped xyz image.\n",
    "    - method: A string representing the method to use to calculate the center of the fruit. Must be either \"median\" or \"mean\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the distance from the camera to the center of the fruit.\n",
    "    \"\"\"\n",
    "    return np.sum(np.array(list(xyz_center_of_box(image, method)))**2)\n",
    "\n",
    "def depth_to_box_center(image, method=\"median\"):\n",
    "    \"\"\"\n",
    "    Calculates the depth from the camera to the center of the fruit.\n",
    "\n",
    "    Args:\n",
    "    - image: A 3D Numpy array representing a cropped xyz image.\n",
    "    - method: A string representing the method to use to calculate the center of the fruit. Must be either \"median\" or \"mean\".\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the depth from the camera to the center of the fruit.\n",
    "    \"\"\"\n",
    "    return xyz_center_of_box(image, method)[2]\n",
    "\n",
    "\n",
    "def hue_filtering(rgb_crop, nstds = 1):\n",
    "    \"\"\"\n",
    "    Apply hue filtering to an RGB image crop.\n",
    "\n",
    "    Parameters:\n",
    "        rgb_crop (numpy.ndarray): Input RGB image crop as a numpy array.\n",
    "        nstds (float): Number of standard deviations used to determine the upper and lower hue thresholds.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: mask where True value indicated what was filtered out\n",
    "    \"\"\"\n",
    "    rgb_c = rgb_crop.copy()\n",
    "    hsv = cv2.cvtColor(rgb_c, cv2.COLOR_RGB2HSV)\n",
    "    hue, sat, v = cv2.split(hsv.copy())\n",
    "    hist_vals, hist_bins = np.histogram(hue, bins = 50)\n",
    "    mode = hist_bins[np.argmax(hist_vals)]\n",
    "    if mode > 35: # greener area\n",
    "        nstds *= 1.5\n",
    "    hue_std = np.std(hue)\n",
    "    upper_limit = mode + nstds*hue_std\n",
    "    lower_limit = mode - nstds*hue_std\n",
    "    logical_vec = np.any([hue > upper_limit, hue < lower_limit], axis = 0)\n",
    "    return logical_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_id_frames(measures_frame, track_id):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of frames associated with a given track ID.\n",
    "\n",
    "    Parameters:\n",
    "    measures_frame (pandas.DataFrame): A dataframe containing measurements data.\n",
    "    track_id (int): The ID of the track to retrieve frames for.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional array of frames associated with the given track ID.\n",
    "    \"\"\"\n",
    "    return measures_frame[measures_frame[\"track_id\"] == track_id][\"frame\"].values\n",
    "\n",
    "def get_track_id_boxes(measures_frame, track_id):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of bounding boxes associated with a given track ID.\n",
    "\n",
    "    Parameters:\n",
    "    measures_frame (pandas.DataFrame): A dataframe containing measurements data.\n",
    "    track_id (int): The ID of the track to retrieve bounding boxes for.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 2-dimensional array of bounding boxes associated with the given track ID.\n",
    "        Each row contains the x1, y1, x2, y2 coordinates of a bounding box in the format [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    return measures_frame[measures_frame[\"track_id\"] == track_id][[\"x1\", \"y1\", \"x2\", \"y2\"]].values\n",
    "\n",
    "def validate_bbox(crop, rgb_img):\n",
    "    \"\"\"\n",
    "    Validates the given bounding box coordinates and ensures that they fall within the dimensions of the RGB image.\n",
    "\n",
    "    Parameters:\n",
    "    crop (tuple): A tuple containing the coordinates of the bounding box in the format (x1, y1, x2, y2).\n",
    "        x1 and y1 are the coordinates of the top-left corner of the bounding box, and x2 and y2 are the coordinates of the bottom-right corner.\n",
    "    rgb_img (numpy.ndarray): A numpy array representing the RGB image.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the validated bounding box coordinates in the format (x1, y1, x2, y2).\n",
    "        The returned coordinates ensure that the bounding box falls entirely within the dimensions of the RGB image.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = crop\n",
    "    x1 = max(x1, 0)\n",
    "    y1 = max(y1, 0)\n",
    "    h, w = rgb_img.shape[:2]\n",
    "    x2 = min(x2, w-1)\n",
    "    y2 = min(y2, h-1)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def get_track_id_images(measures_frame, track_id, args):\n",
    "    cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)\n",
    "    frame_numbers = get_track_id_frames(measures_frame, track_id)\n",
    "    boxes = get_track_id_boxes(measures_frame, track_id)\n",
    "    rgb_images, pc_images = [], []\n",
    "    for i in tqdm(range(len(frame_numbers))):\n",
    "        frame, crop = frame_numbers[i], boxes[i]\n",
    "        rgb_img, _, pc_img = cam.get_zed(frame)\n",
    "        x1,y1,x2,y2 = validate_bbox(crop, rgb_img)\n",
    "        rgb_images.append(rgb_img[y1:y2,x1:x2, ::-1])\n",
    "        pc_images.append(pc_img[y1:y2,x1:x2])\n",
    "    cam.close()\n",
    "    return rgb_images, pc_images\n",
    "\n",
    "def kde_filtering(centers, thresh=0.5):\n",
    "    \"\"\"\n",
    "    Applies a Kernel Density Estimation (KDE) filtering on a set of 3D points.\n",
    "\n",
    "    Args:\n",
    "        centers (np.ndarray): A numpy array of shape (n, 3) representing the 3D coordinates of the points to filter.\n",
    "        thresh (float): A threshold value to filter out points with low density. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of shape (n, 3) where each filtered 3D points is replaced with np.nan\n",
    "    \"\"\"\n",
    "    finite_logical = np.all(np.isfinite(centers), axis=1)\n",
    "    if not sum(finite_logical):\n",
    "        return centers\n",
    "    finite_centers = centers[finite_logical].copy()\n",
    "    kernel = gaussian_kde(finite_centers.T)\n",
    "    points_density = np.full(len(centers), np.nan)\n",
    "    points_density[finite_logical] = kernel(finite_centers.T)\n",
    "    points_density = points_density/np.nansum(points_density)\n",
    "    filtered_center = centers.copy()\n",
    "    filtered_center[points_density <thresh/ len(finite_logical)] = np.nan\n",
    "    return filtered_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb_images, pc_images = get_track_id_images(measures_frame, 7)\n",
    "# plot_2_imgs(rgb_images[0], pc_images[0][:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_xyz_outliers(crop, nstd=2, as_points=True):\n",
    "    \"\"\"\n",
    "    Filters out the outliers from the 3D points in the given crop.\n",
    "\n",
    "    Args:\n",
    "        crop (ndarray): A numpy array of shape (height, width, 3) containing the 3D points.\n",
    "        nstd (float): The number of standard deviations to consider for defining the range of valid values.\n",
    "        as_points (bool): Whether to return the filtered 3D points as an array of points or as an array of the same shape\n",
    "            as the input crop.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: A numpy array of filtered 3D points. If as_points is True, this is a numpy array of shape (n, 3),\n",
    "            where n is the number of valid 3D points. Otherwise, it is a numpy array of the same shape as the input crop.\n",
    "    \"\"\"\n",
    "    centers = crop.reshape(-1, 3)\n",
    "    filtered_centers = centers.copy()\n",
    "    channel_medians = np.nanmedian(centers, axis=0)\n",
    "    channel_stddevs = np.nanstd(centers, axis=0)\n",
    "    channel_max = channel_medians + nstd * channel_stddevs\n",
    "    channel_min = channel_medians - nstd * channel_stddevs\n",
    "    valid_mask = np.all((centers >= channel_min) & (centers <= channel_max), axis=1)\n",
    "    filtered_centers[~valid_mask] = np.nan\n",
    "    if as_points:\n",
    "        return filtered_centers\n",
    "    return filtered_centers.reshape(crop.shape)\n",
    "\n",
    "\n",
    "def ellipsoid_fit(filtered_center):\n",
    "    \"\"\"\n",
    "    Fits an ellipsoid to a set of 3D points using least squares estimation.\n",
    "\n",
    "    Args:\n",
    "    - filtered_center (numpy array): An N x 3 array of N 3D points in the form (x, y, z).\n",
    "\n",
    "    Returns:\n",
    "    - radius (float): The radius of the fitted ellipsoid.\n",
    "    - semi_axis_1 (float): The length of the semi-major axis of the ellipsoid.\n",
    "    - semi_axis_2 (float): The length of the semi-intermediate axis of the ellipsoid.\n",
    "    - semi_axis_3 (float): The length of the semi-minor axis of the ellipsoid.\n",
    "    \"\"\"\n",
    "    filtered_center[np.abs(filtered_center) > 2] = np.nan\n",
    "    A = np.column_stack([filtered_center, np.ones(len(filtered_center))])\n",
    "    good_rows = np.all(np.isfinite(A), axis=1)\n",
    "    A = A[good_rows]\n",
    "\n",
    "    #   Assemble the f matrix\n",
    "    f = np.zeros((len(A),1))\n",
    "    f[:,0] = np.sum(A[:,:3]**2, axis = 1)\n",
    "    C, residules, rank, singval = np.linalg.lstsq(A,f)\n",
    "    C = np.abs(C)\n",
    "    #   solve for the radius\n",
    "    t = (C[0]*C[0])+(C[1]*C[1])+(C[2]*C[2])+C[3]\n",
    "    radius = np.sqrt(t)\n",
    "    # channels are switched\n",
    "    return radius, np.sqrt(C[1]), np.sqrt(C[0]), np.sqrt(C[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "def get_cropped_point_cloud(bbox, point_cloud, margin=0.2):\n",
    "    crop = point_cloud[max(int(bbox[1]), 0):int(bbox[3]), max(int(bbox[0]), 0): int(bbox[2]), :-1].copy()\n",
    "    return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vision.visualization.drawer import draw_rectangle, draw_text, draw_highlighted_test, get_color\n",
    "def get_color(rgb_crop):\n",
    "    if not len(rgb_crop):\n",
    "        return (255,255,255)\n",
    "    hsv = cv2.cvtColor(rgb_crop.astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "    hue, sat, val = cv2.split(hsv.copy())\n",
    "    mean_hue, median_hue = np.nanmean(hue), np.nanmedian(hue)\n",
    "    hist_vals, hist_bins = np.histogram(hue, bins = 100)\n",
    "    mode_hue = hist_bins[np.argmax(hist_vals)]\n",
    "    if mode_hue < 10:\n",
    "        return (255,0,0)\n",
    "    if mode_hue < 17.5:\n",
    "        return (255,125,0)\n",
    "    if mode_hue < 45:\n",
    "        w, h = rgb_crop.shape[:2]\n",
    "        w_025 = int(w/4)\n",
    "        h_025 = int(h/4)\n",
    "        hue_cut = hue[h_025:-h_025,w_025:-w_025]\n",
    "        mean_hue_cut, median_hue_cut = np.nanmean(hue_cut), np.nanmedian(hue_cut)\n",
    "        if median_hue_cut * 1.15 > mean_hue_cut: # low \"skew\"\n",
    "            return (255,255,0)\n",
    "        else:\n",
    "            return (128,0,128)\n",
    "    return (0,255,0)\n",
    "\n",
    "\n",
    "def draw_dets(frame, dets, t_index=6):\n",
    "    out_frame = frame.copy()\n",
    "    for det in dets:\n",
    "        x1,y1,x2,y2 = validate_bbox(det[:4].astype(np.int), frame)\n",
    "        track_id = det[t_index]\n",
    "        color_id = int(track_id) % 15  # 15 is the number of colors in list\n",
    "        color = get_color(frame[y1:y2, x1:x2])\n",
    "        text_color = (0,0,0)\n",
    "        out_frame = draw_rectangle(out_frame, (int(det[0]), int(det[1])), (int(det[2]), int(det[3])), color, 3)\n",
    "        out_frame = draw_highlighted_test(out_frame, f'ID:{track_id}', (det[0], det[1]), frame.shape[1],\n",
    "                                          color, text_color, True, 10, 3)\n",
    "\n",
    "    return out_frame\n",
    "cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)\n",
    "for frame_number in range(50,800, 5):\n",
    "    rgb_img, _, pc_img = cam.get_zed(frame_number)\n",
    "    if isinstance(rgb_img, type(None)):\n",
    "        continue\n",
    "    rgb_img = rgb_img[:,:,::-1]\n",
    "    boxes_for_frame = measures_frame[measures_frame[\"frame\"]==frame_number].values\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(draw_dets(rgb_img.astype(np.uint8), boxes_for_frame)[900:-300,:,:])\n",
    "    plt.title(frame_number)\n",
    "    plt.show()\n",
    "cam.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# green_fruits = [53, 71, 131 ,108, 91, 228, 225 ,247, 442, 452, 502,943]\n",
    "# green_fruits_sat = [61,70]\n",
    "# breaking_fruits = [227, 163, 217, 851, 260, 304,1220, 1752, 304,450,2397]\n",
    "# yellow_fruits = [82, 95, 739 ,856, 959, 1149, 1530,1645]\n",
    "# oragne_fruits = [246, 451, 410, 1112, 1235, 1464,1525, 1911, 2298, 1464]\n",
    "# red_fruits = [90, 558,515, 1687, 1706, 1910, 558,1129, 1515, 1778,1905, 2394]\n",
    "\n",
    "# fruit_dict = {\n",
    "#     \"red\": red_fruits,\n",
    "#     \"orange\": oragne_fruits,\n",
    "#     \"yellow\": yellow_fruits,\n",
    "#     \"breaking\": breaking_fruits,\n",
    "#     \"green\": green_fruits\n",
    "# }\n",
    "\n",
    "# # for track_id in [6,7,12,28]:\n",
    "# cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)\n",
    "# for fruit_color, track_ids in fruit_dict.items():\n",
    "#     for track_id in track_ids:\n",
    "#         frame_numbers = get_track_id_frames(measures_frame, track_id)\n",
    "#         boxes = get_track_id_boxes(measures_frame, track_id)\n",
    "\n",
    "#         for i in tqdm(range(len(frame_numbers))):\n",
    "#             frame, box = frame_numbers[i], boxes[i]\n",
    "#             rgb_img, _, pc_img = cam.get_zed(frame)\n",
    "#             x1,y1,x2,y2 = validate_bbox(box, rgb_img)\n",
    "#             pc_crop = pc_img[y1:y2, x1:x2,:3]\n",
    "#             rgb_crop = rgb_img[y1:y2, x1:x2, ::-1]\n",
    "#             hsv = cv2.cvtColor(rgb_crop, cv2.COLOR_RGB2HSV)\n",
    "#             hue, sat, val = cv2.split(hsv.copy())\n",
    "#         mean_hue, median_hue = np.nanmean(hue), np.nanmedian(hue)\n",
    "#         hist_vals, hist_bins = np.histogram(hue, bins = 50)\n",
    "#         mode_hue = hist_bins[np.argmax(hist_vals)]\n",
    "#         title = f\"t_id: {track_id}, fruit color: {fruit_color}, mean hue:{mean_hue} , median hue:{median_hue}, mode hue:{mode_hue}\"\n",
    "#         plot_2_imgs(rgb_crop, hue, title)\n",
    "# cam.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53, 163, 471, 1112, 1910\n",
    "track_id = 227\n",
    "cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)\n",
    "frame_numbers = get_track_id_frames(measures_frame, track_id)\n",
    "i = len(frame_numbers) -1\n",
    "frame, box = frame_numbers[i], boxes[i]\n",
    "rgb_img, _, pc_img = cam.get_zed(frame)\n",
    "x1,y1,x2,y2 = validate_bbox(box, rgb_img)\n",
    "pc_crop = pc_img[y1:y2, x1:x2,:3]\n",
    "plt.imshow(rgb_img[y1:y2, x1:x2, ::-1])\n",
    "plt.title(track_id)\n",
    "plt.show()\n",
    "cam.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_color(rgb_crop):\n",
    "    if not len(rgb_crop):\n",
    "        return (255,255,255)\n",
    "    hsv = cv2.cvtColor(rgb_crop.astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "    hue, sat, val = cv2.split(hsv.copy())\n",
    "    mean_hue, median_hue = np.nanmean(hue), np.nanmedian(hue)\n",
    "    hist_vals, hist_bins = np.histogram(hue, bins = 100)\n",
    "    mode_hue = hist_bins[np.argmax(hist_vals)]\n",
    "    if mode_hue < 10:\n",
    "        return (255,0,0)\n",
    "    if mode_hue < 17.5:\n",
    "        return (255,125,0)\n",
    "    if mode_hue < 45:\n",
    "        w, h = rgb_crop.shape[:2]\n",
    "        w_025 = int(w/4)\n",
    "        h_025 = int(h/4)\n",
    "        hue_cut = hue[h_025:-h_025,w_025:-w_025]\n",
    "        mean_hue_cut, median_hue_cut = np.nanmean(hue_cut), np.nanmedian(hue_cut)\n",
    "        if median_hue_cut * 1.15 > mean_hue_cut: # low \"skew\"\n",
    "            return (255,255,0)\n",
    "        else:\n",
    "            return (128,0,128)\n",
    "    return (0,255,0)\n",
    "\n",
    "track_id = 1443\n",
    "cam = video_wrapper(args.movie_path, args.rotate, args.depth_minimum, args.depth_maximum)\n",
    "frame_numbers = get_track_id_frames(measures_frame, track_id)\n",
    "boxes = get_track_id_boxes(measures_frame, track_id)\n",
    "i = 7\n",
    "for i in range(len(frame_numbers)):\n",
    "    frame, box = frame_numbers[i], boxes[i]\n",
    "    rgb_img, _, pc_img = cam.get_zed(frame)\n",
    "    x1,y1,x2,y2 = validate_bbox(box, rgb_img)\n",
    "    pc_crop = pc_img[y1:y2, x1:x2,:3]\n",
    "    rgb_crop = rgb_img[y1:y2, x1:x2, ::-1]\n",
    "    rgb_c = rgb_crop.copy()\n",
    "    w, h = rgb_crop.shape[:2]\n",
    "    w_025 = int(w/4)\n",
    "    h_025 = int(h/4)\n",
    "    hsv = cv2.cvtColor(rgb_crop, cv2.COLOR_RGB2HSV)\n",
    "    hue, sat, val = cv2.split(hsv.copy())\n",
    "    mean_hue, median_hue = np.nanmean(hue), np.nanmedian(hue)\n",
    "    hist_vals, hist_bins = np.histogram(hue, bins = 100)\n",
    "    mode_hue = hist_bins[np.argmax(hist_vals)]\n",
    "    hue_cut = hue[h_025:-h_025,w_025:-w_025]\n",
    "    mean_hue_cut, median_hue_cut = np.nanmean(hue_cut), np.nanmedian(hue_cut)\n",
    "    hist_vals_cut, hist_bins_cut = np.histogram(hue_cut, bins = 100)\n",
    "    mode_hue_cut = hist_bins_cut[np.argmax(hist_vals_cut)]\n",
    "    \n",
    "    color = get_color(rgb_crop)\n",
    "    rgb_crop[0,:] = color\n",
    "    rgb_crop[-1,:] = color\n",
    "    rgb_crop[:,0] = color\n",
    "    rgb_crop[:,-1] = color\n",
    "    print(f\"\"\"mean:, {mean_hue}, median: {median_hue}, mode: {mode_hue}, std: {np.nanstd(hue)} \\n\n",
    "          mean:, {mean_hue_cut}, median: {median_hue_cut}, mode: {mode_hue_cut}, std: {np.nanstd(hue_cut)}\"\"\")\n",
    "    plt.hist(hue.flatten(), bins = 50,density = True)\n",
    "    plt.show()\n",
    "    plt.imshow(rgb_crop)\n",
    "    plt.title(frame)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_box_pc = cut_center_of_box(pc_crop, 0.1)\n",
    "cut_box_rgb = cut_center_of_box(rgb_img[y1:y2, x1:x2], 0.1)\n",
    "plot_2_imgs(cut_box_pc[:, :, 2], cut_box_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobol(det_crop, plot_change=False):\n",
    "    \"\"\"\n",
    "    applies sobol filterning on image\n",
    "    :param det_crop: image to apply filter on\n",
    "    :param plot_change: flag to show the image after applying sobol\n",
    "    :return: image after sobol filtering\n",
    "    \"\"\"\n",
    "    torch_img = K.utils.image_to_tensor(det_crop)\n",
    "    torch_img = torch_img[None, ...].float() / 255.\n",
    "    torch_img = K.enhance.adjust_contrast(torch_img, 0.5)\n",
    "    torch_img_gray = K.color.rgb_to_grayscale(torch_img)\n",
    "    processed_img = K.filters.sobel(torch_img_gray, True, 1e-3) \n",
    "    if plot_change:\n",
    "        plot_2_imgs(det_crop, processed_img.detach().numpy()[0, 0] > 0.05)\n",
    "    return processed_img.detach().numpy()[0, 0]\n",
    "out_img = apply_sobol(rgb_img[y1:y2, x1:x2, ::-1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_area = 5\n",
    "out_img = out_img > 0.04\n",
    "rgb_crop = rgb_img[y1:y2, x1:x2, ::-1].copy()\n",
    "contours, _ = cv2.findContours(out_img.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "filtered_contours = []\n",
    "for c in contours:\n",
    "    if cv2.contourArea(c) >= min_area:\n",
    "        filtered_contours.append(c)\n",
    "\n",
    "mask = np.zeros_like(out_img).astype(np.uint8)\n",
    "hull = cv2.convexHull(np.concatenate(filtered_contours))\n",
    "cv2.drawContours(mask, [hull], 0, (255, 255, 255), -1)\n",
    "plot_2_imgs(out_img, mask)\n",
    "\n",
    "rgb_c = rgb_crop.copy()\n",
    "rgb_c[np.logical_not(mask)] = 0\n",
    "plot_2_imgs(rgb_crop, rgb_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2_imgs(rgb_crop, hue_filtering(rgb_crop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins[np.argmax(hist_vals)] + 1*np.std(hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = cv2.cvtColor(rgb_c, cv2.COLOR_RGB2HSV)\n",
    "hue, sat, v = cv2.split(hsv.copy())\n",
    "rgb_crop = rgb_c.copy()\n",
    "plot_2_imgs(rgb_c, hue)\n",
    "rgb_crop[hue > 200] = 0\n",
    "plot_2_imgs(rgb_c, rgb_crop)\n",
    "rgb_crop[hue > 150] = 0\n",
    "plot_2_imgs(rgb_c, rgb_crop)\n",
    "rgb_crop[hue > 100] = 0\n",
    "plot_2_imgs(rgb_c, rgb_crop)\n",
    "rgb_crop[hue > 50] = 0\n",
    "plot_2_imgs(rgb_c, rgb_crop)\n",
    "rgb_crop[hue > 25] = 0\n",
    "plot_2_imgs(rgb_c, rgb_crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FsCounter",
   "language": "python",
   "name": "fscounter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
